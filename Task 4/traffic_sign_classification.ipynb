{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b6c580c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic Libraries\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# PyTorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import transforms, datasets, models\n",
    "\n",
    "# Scikit-learn\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n",
    "from PIL import Image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "63a4fecc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "# Dataset root folder\n",
    "DATA_ROOT = \"./GTSRB\"\n",
    "TRAIN_DIR = os.path.join(DATA_ROOT, \"Train\")\n",
    "TEST_DIR  = os.path.join(DATA_ROOT, \"Test\")\n",
    "\n",
    "TRAIN_CSV = os.path.join(DATA_ROOT, \"Train.csv\")\n",
    "TEST_CSV  = os.path.join(DATA_ROOT, \"Test.csv\")\n",
    "\n",
    "# Device (GPU if available)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "30b36fc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data augmentations for training\n",
    "train_transforms = transforms.Compose([\n",
    "    transforms.Resize((64, 64)),\n",
    "    transforms.RandomRotation(10),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "\n",
    "# No augmentation for test set\n",
    "test_transforms = transforms.Compose([\n",
    "    transforms.Resize((64, 64)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "15a0f5e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of classes: 43\n"
     ]
    }
   ],
   "source": [
    "train_dataset = datasets.ImageFolder(root=TRAIN_DIR, transform=train_transforms)\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True, num_workers=2)\n",
    "\n",
    "num_classes = len(train_dataset.classes)\n",
    "print(\"Number of classes:\", num_classes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8f5ff222",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test samples: 12630\n"
     ]
    }
   ],
   "source": [
    "class TrafficSignTestDataset(Dataset):\n",
    "    def __init__(self, csv_file, root_dir, transform=None):\n",
    "        self.data = pd.read_csv(csv_file)\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.data.iloc[idx]\n",
    "        img_path = os.path.join(self.root_dir, os.path.basename(row[\"Path\"]))\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        label = int(row[\"ClassId\"])\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, label\n",
    "\n",
    "test_dataset = TrafficSignTestDataset(TEST_CSV, TEST_DIR, transform=test_transforms)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "print(\"Test samples:\", len(test_dataset))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4b84cd88",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrafficSignCNN(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(TrafficSignCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.fc1 = nn.Linear(64 * 16 * 16, 128)\n",
    "        self.fc2 = nn.Linear(128, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 64 * 16 * 16)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "custom_cnn = TrafficSignCNN(num_classes=num_classes).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer_cnn = optim.Adam(custom_cnn.parameters(), lr=0.001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f0cc46b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Custom CNN] Epoch [1/10], Loss: 1.0414\n",
      "[Custom CNN] Epoch [2/10], Loss: 0.2691\n",
      "[Custom CNN] Epoch [3/10], Loss: 0.1627\n",
      "[Custom CNN] Epoch [4/10], Loss: 0.1209\n",
      "[Custom CNN] Epoch [5/10], Loss: 0.0916\n",
      "[Custom CNN] Epoch [6/10], Loss: 0.0781\n",
      "[Custom CNN] Epoch [7/10], Loss: 0.0650\n",
      "[Custom CNN] Epoch [8/10], Loss: 0.0576\n",
      "[Custom CNN] Epoch [9/10], Loss: 0.0525\n",
      "[Custom CNN] Epoch [10/10], Loss: 0.0488\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 10\n",
    "for epoch in range(EPOCHS):\n",
    "    custom_cnn.train()\n",
    "    running_loss = 0.0\n",
    "    for images, labels in train_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        optimizer_cnn.zero_grad()\n",
    "        outputs = custom_cnn(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer_cnn.step()\n",
    "        running_loss += loss.item()\n",
    "    print(f\"[Custom CNN] Epoch [{epoch+1}/{EPOCHS}], Loss: {running_loss/len(train_loader):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ce760d37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report (Custom CNN):\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.5795    0.8500    0.6892        60\n",
      "           1     0.9354    0.8250    0.8768       720\n",
      "           2     0.0000    0.0000    0.0000       750\n",
      "           3     0.0024    0.0022    0.0023       450\n",
      "           4     0.0000    0.0000    0.0000       660\n",
      "           5     0.0000    0.0000    0.0000       630\n",
      "           6     0.0000    0.0000    0.0000       150\n",
      "           7     0.0000    0.0000    0.0000       450\n",
      "           8     0.0000    0.0000    0.0000       450\n",
      "           9     0.0000    0.0000    0.0000       480\n",
      "          10     0.0037    0.0015    0.0022       660\n",
      "          11     0.0000    0.0000    0.0000       420\n",
      "          12     0.0000    0.0000    0.0000       690\n",
      "          13     0.0000    0.0000    0.0000       720\n",
      "          14     0.0000    0.0000    0.0000       270\n",
      "          15     0.0000    0.0000    0.0000       210\n",
      "          16     0.0000    0.0000    0.0000       150\n",
      "          17     0.0000    0.0000    0.0000       360\n",
      "          18     0.1147    0.2026    0.1464       390\n",
      "          19     0.0000    0.0000    0.0000        60\n",
      "          20     0.0000    0.0000    0.0000        90\n",
      "          21     0.0000    0.0000    0.0000        90\n",
      "          22     0.0081    0.0083    0.0082       120\n",
      "          23     0.0000    0.0000    0.0000       150\n",
      "          24     0.0000    0.0000    0.0000        90\n",
      "          25     0.0000    0.0000    0.0000       480\n",
      "          26     0.0000    0.0000    0.0000       180\n",
      "          27     0.0000    0.0000    0.0000        60\n",
      "          28     0.0000    0.0000    0.0000       150\n",
      "          29     0.0000    0.0000    0.0000        90\n",
      "          30     0.0000    0.0000    0.0000       150\n",
      "          31     0.0000    0.0000    0.0000       270\n",
      "          32     0.0000    0.0000    0.0000        60\n",
      "          33     0.0000    0.0000    0.0000       210\n",
      "          34     0.0000    0.0000    0.0000       120\n",
      "          35     0.0000    0.0000    0.0000       390\n",
      "          36     0.1667    0.1083    0.1313       120\n",
      "          37     0.0000    0.0000    0.0000        60\n",
      "          38     0.0000    0.0000    0.0000       690\n",
      "          39     0.0000    0.0000    0.0000        90\n",
      "          40     0.0000    0.0000    0.0000        90\n",
      "          41     0.0000    0.0000    0.0000        60\n",
      "          42     0.0000    0.0000    0.0000        90\n",
      "\n",
      "    accuracy                         0.0586     12630\n",
      "   macro avg     0.0421    0.0465    0.0432     12630\n",
      "weighted avg     0.0616    0.0586    0.0593     12630\n",
      "\n"
     ]
    }
   ],
   "source": [
    "custom_cnn.eval()\n",
    "y_true_cnn, y_pred_cnn = [], []\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = custom_cnn(images)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        y_true_cnn.extend(labels.cpu().numpy())\n",
    "        y_pred_cnn.extend(preds.cpu().numpy())\n",
    "\n",
    "print(\"Classification Report (Custom CNN):\\n\")\n",
    "print(classification_report(y_true_cnn, y_pred_cnn, digits=4))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6f89e97",
   "metadata": {},
   "source": [
    "# Pretrained MobileNetV2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3f4a1179",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mohsin/miniconda/envs/dl_env/lib/python3.11/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/mohsin/miniconda/envs/dl_env/lib/python3.11/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=MobileNet_V2_Weights.IMAGENET1K_V1`. You can also use `weights=MobileNet_V2_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "mobilenet = models.mobilenet_v2(pretrained=True)\n",
    "for param in mobilenet.features.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "mobilenet.classifier[1] = nn.Linear(1280, num_classes)\n",
    "mobilenet = mobilenet.to(device)\n",
    "\n",
    "optimizer_mobilenet = optim.Adam(mobilenet.classifier.parameters(), lr=0.001)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30b0e805",
   "metadata": {},
   "source": [
    "# Train MonileNetV2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5d0f98fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[MobileNetV2] Epoch [1/5], Loss: 1.7907\n",
      "[MobileNetV2] Epoch [2/5], Loss: 1.4365\n",
      "[MobileNetV2] Epoch [3/5], Loss: 1.3785\n",
      "[MobileNetV2] Epoch [4/5], Loss: 1.3332\n",
      "[MobileNetV2] Epoch [5/5], Loss: 1.3173\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 5\n",
    "for epoch in range(EPOCHS):\n",
    "    mobilenet.train()\n",
    "    running_loss = 0.0\n",
    "    for images, labels in train_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        optimizer_mobilenet.zero_grad()\n",
    "        outputs = mobilenet(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer_mobilenet.step()\n",
    "        running_loss += loss.item()\n",
    "    print(f\"[MobileNetV2] Epoch [{epoch+1}/{EPOCHS}], Loss: {running_loss/len(train_loader):.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f93873c5",
   "metadata": {},
   "source": [
    "# Evaluate MobileNetV2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fffccbd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report (MobileNetV2):\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.4615    0.2000    0.2791        60\n",
      "           1     0.5386    0.5042    0.5208       720\n",
      "           2     0.0125    0.0093    0.0107       750\n",
      "           3     0.0000    0.0000    0.0000       450\n",
      "           4     0.0142    0.0167    0.0153       660\n",
      "           5     0.0000    0.0000    0.0000       630\n",
      "           6     0.0000    0.0000    0.0000       150\n",
      "           7     0.0047    0.0022    0.0030       450\n",
      "           8     0.0061    0.0022    0.0033       450\n",
      "           9     0.0355    0.0271    0.0307       480\n",
      "          10     0.0000    0.0000    0.0000       660\n",
      "          11     0.2500    0.0024    0.0047       420\n",
      "          12     0.0029    0.0029    0.0029       690\n",
      "          13     0.0185    0.0014    0.0026       720\n",
      "          14     0.0000    0.0000    0.0000       270\n",
      "          15     0.0000    0.0000    0.0000       210\n",
      "          16     0.0000    0.0000    0.0000       150\n",
      "          17     0.0000    0.0000    0.0000       360\n",
      "          18     0.0786    0.0821    0.0803       390\n",
      "          19     0.0818    0.1500    0.1059        60\n",
      "          20     0.0000    0.0000    0.0000        90\n",
      "          21     0.0330    0.0778    0.0464        90\n",
      "          22     0.0743    0.1250    0.0932       120\n",
      "          23     0.0000    0.0000    0.0000       150\n",
      "          24     0.0000    0.0000    0.0000        90\n",
      "          25     0.1145    0.1021    0.1079       480\n",
      "          26     0.0000    0.0000    0.0000       180\n",
      "          27     0.0000    0.0000    0.0000        60\n",
      "          28     0.0000    0.0000    0.0000       150\n",
      "          29     0.0000    0.0000    0.0000        90\n",
      "          30     0.0000    0.0000    0.0000       150\n",
      "          31     0.0000    0.0000    0.0000       270\n",
      "          32     0.0009    0.0167    0.0016        60\n",
      "          33     0.4545    0.0476    0.0862       210\n",
      "          34     0.0030    0.0250    0.0053       120\n",
      "          35     0.0455    0.0179    0.0257       390\n",
      "          36     0.0000    0.0000    0.0000       120\n",
      "          37     0.1379    0.0667    0.0899        60\n",
      "          38     0.0044    0.0043    0.0044       690\n",
      "          39     0.0000    0.0000    0.0000        90\n",
      "          40     0.0000    0.0000    0.0000        90\n",
      "          41     0.0000    0.0000    0.0000        60\n",
      "          42     0.0029    0.0111    0.0046        90\n",
      "\n",
      "    accuracy                         0.0438     12630\n",
      "   macro avg     0.0553    0.0348    0.0355     12630\n",
      "weighted avg     0.0637    0.0438    0.0456     12630\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mobilenet.eval()\n",
    "y_true_mobilenet, y_pred_mobilenet = [], []\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = mobilenet(images)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        y_true_mobilenet.extend(labels.cpu().numpy())\n",
    "        y_pred_mobilenet.extend(preds.cpu().numpy())\n",
    "\n",
    "print(\"Classification Report (MobileNetV2):\\n\")\n",
    "print(classification_report(y_true_mobilenet, y_pred_mobilenet, digits=4))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7b1a5a7",
   "metadata": {},
   "source": [
    "# Comparison Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4767821e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Comparison Table:\n",
      "          Model  Test Accuracy\n",
      "0   Custom CNN       0.058591\n",
      "1  MobileNetV2       0.043785\n"
     ]
    }
   ],
   "source": [
    "custom_cnn_accuracy = accuracy_score(y_true_cnn, y_pred_cnn)\n",
    "mobilenet_accuracy  = accuracy_score(y_true_mobilenet, y_pred_mobilenet)\n",
    "\n",
    "results = {\n",
    "    \"Model\": [\"Custom CNN\", \"MobileNetV2\"],\n",
    "    \"Test Accuracy\": [custom_cnn_accuracy, mobilenet_accuracy]\n",
    "}\n",
    "df = pd.DataFrame(results)\n",
    "print(\"\\nComparison Table:\\n\", df)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
